# StatefulSet for Distributed Training
#
# This example shows how to use StatefulSet for distributed LLM training.
# Each pod has a stable rank (0, 1, 2, ...) derived from its pod name.
#
# Key concepts:
# - StatefulSet: Stable identity for distributed training
# - Rank extraction: Extract rank from pod name (web-0 → rank 0)
# - Headless service: DNS-based pod discovery
# - Per-pod PVCs: Checkpoint storage
#
# For LLM training:
# - PyTorch DDP / DeepSpeed requires stable ranks
# - Pod 0 is the master/primary
# - Other pods are workers
# - Checkpoints stored on per-pod PVCs
#
# Usage:
#   kubectl apply -f 02-distributed-training.yaml
#   kubectl delete -f 02-distributed-training.yaml

---
# Headless Service for StatefulSet
apiVersion: v1
kind: Service
metadata:
  name: training
  labels:
    app: distributed-training
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None  # Headless service
  selector:
    app: distributed-training

---
# StatefulSet for distributed training
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: trainer
  labels:
    app: distributed-training
spec:
  serviceName: "training"  # Must match headless service
  replicas: 3  # 3-way distributed training
  selector:
    matchLabels:
      app: distributed-training
  template:
    metadata:
      labels:
        app: distributed-training
    spec:
      containers:
      - name: trainer
        image: python:3.11-slim
        command: ["/bin/sh", "-c"]
        args:
        - |
          # Extract rank from pod name (trainer-0 → rank 0)
          POD_NAME=$(hostname)
          RANK=$(echo $POD_NAME | rev | cut -d- -f1 | rev)
          MASTER_ADDR="trainer-0.training"  # DNS name of rank 0

          echo "==================================="
          echo "Distributed Training Setup"
          echo "==================================="
          echo "Pod: $POD_NAME"
          echo "Rank: $RANK"
          echo "Master: $MASTER_ADDR"
          echo "World Size: 3"
          echo "==================================="
          echo ""

          # Create readiness marker file
          touch /tmp/ready

          # Create a simple training simulation script
          cat > /tmp/training.py << 'EOF'
          import os
          import time
          from datetime import datetime

          rank = int(os.environ.get("RANK", "0"))
          master = os.environ.get("MASTER_ADDR", "trainer-0.training")

          print(f"Starting training worker with rank {rank}")
          print(f"Master address: {master}")
          print(f"World size: 3")

          # Simulate training loop
          for step in range(1, 4):
              print(f"[Rank {rank}] Step {step}: Training...")
              time.sleep(2)

              # Simulate checkpoint
              checkpoint_file = f"/checkpoints/checkpoint_step{step}_rank{rank}.pt"
              with open(checkpoint_file, "w") as f:
                  f.write(f"Checkpoint at step {step}, rank {rank}, time {datetime.now()}")
              print(f"[Rank {rank}] Saved checkpoint: {checkpoint_file}")

          print(f"[Rank {rank}] Training complete!")

          # Keep container running
          while True:
              time.sleep(60)
          EOF

          # Export environment variables for training script
          export RANK=$RANK
          export MASTER_ADDR=$MASTER_ADDR
          export WORLD_SIZE=3

          # Run training
          python /tmp/training.py
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        ports:
        - containerPort: 80
          name: web
        # Readiness probe - check if ready file exists
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/ready
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 2
          failureThreshold: 3

        volumeMounts:
        - name: checkpoints
          mountPath: /checkpoints
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
  volumeClaimTemplates:
  - metadata:
      name: checkpoints
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi

# Training Setup:
#
# Pod Naming:
#   - trainer-0: Rank 0 (master/primary)
#   - trainer-1: Rank 1 (worker)
#   - trainer-2: Rank 2 (worker)
#
# DNS Names:
#   - trainer-0.training.default.svc.cluster.local
#   - trainer-1.training.default.svc.cluster.local
#   - trainer-2.training.default.svc.cluster.local
#
# Checkpoints:
#   - PVC: checkpoints-trainer-0 → mounted at /checkpoints on trainer-0
#   - PVC: checkpoints-trainer-1 → mounted at /checkpoints on trainer-1
#   - PVC: checkpoints-trainer-2 → mounted at /checkpoints on trainer-2
#
# Training Framework Integration:
#
# PyTorch DDP:
#   export MASTER_ADDR=trainer-0.training
#   export MASTER_PORT=29500
#   export WORLD_SIZE=3
#   export RANK=$RANK
#   python -m torch.distributed.launch --nproc_per_node=1 train.py
#
# DeepSpeed:
#   ds_launch --master_addr=trainer-0.training --world_size=3 train.py
#
# Megatron-LM:
#   Distributed launcher handles rank allocation automatically

# Why StatefulSet for Training?
# 1. Stable Identity:
#    - Rank 0 is always trainer-0
#    - Workers can always reach master at trainer-0.training
#    - No race conditions during initialization
#
# 2. Checkpoint Storage:
#    - Each worker has its own checkpoint directory
#    - Checkpoints persist across pod restarts
#    - Resume training from last checkpoint
#
# 3. Ordered Scaling:
#    - Scale up: New workers join with correct ranks
#    - Scale down: Workers exit gracefully
#
# 4. Fault Tolerance:
#    - If worker pod dies, it recreates with same rank
#    - Resume from checkpoint on per-pod PVC
