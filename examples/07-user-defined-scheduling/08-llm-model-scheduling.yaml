# Complete LLM Model Scheduling Example
#
# This example combines ALL the user-defined scheduling techniques we've learned:
# - Node selector (GPU type)
# - Node affinity (zone preference)
# - Pod affinity (co-locate with cache)
# - Pod anti-affinity (spread across nodes)
# - Taints and tolerations (dedicated GPU nodes)
# - Priority classes (production priority)
# - Topology spread constraints (zone distribution)
#
# This is a production-ready configuration for serving LLM models.

---
# Priority Classes
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: llm-production
value: 1000
globalDefault: false
description: "Production LLM serving - highest priority"

---
# ConfigMap for shared configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-3-70b-config
data:
  MODEL_NAME: "meta-llama/Meta-Llama-3-70B"
  TENSOR_PARALLEL_SIZE: "4"
  MAX_MODEL_LEN: "8192"
  DTYPE: "half"
  GPU_MEMORY_UTILIZATION: "0.9"

---
# Service for model serving
apiVersion: v1
kind: Service
metadata:
  name: llama-3-70b-service
  labels:
    app: llama-3-70b
    env: production
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 8000
    targetPort: 8000
    protocol: TCP
  - name: health
    port: 8080
    targetPort: 8080
    protocol: TCP
  selector:
    app: llama-3-70b

---
# Redis cache for model outputs (co-located with model pods)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-3-70b-cache
  labels:
    app: llama-3-70b-cache
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llama-3-70b-cache
  template:
    metadata:
      labels:
        app: llama-3-70b-cache
    spec:
      # Co-locate cache with model pods
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - llama-3-70b
            topologyKey: kubernetes.io/hostname

      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - name: redis
          containerPort: 6379
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 8Gi
        livenessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 5
          periodSeconds: 5

---
# Main LLM model deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-3-70b
  labels:
    app: llama-3-70b
    env: production
    model-type: llm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llama-3-70b
  template:
    metadata:
      labels:
        app: llama-3-70b
        env: production
        model-type: llm
    spec:
      # PRIORITY: Production critical
      priorityClassName: llm-production

      # TOLERATIONS: Allow on dedicated GPU nodes
      tolerations:
      - key: "gpu-only"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      - key: "nvidia.com/gpu.product"
        operator: "Equal"
        value: "H100"
        effect: "NoSchedule"

      # COMPLEX SCHEDULING STRATEGY
      affinity:
        # 1. Node Affinity: Prefer H100 nodes in zone A
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - matchExpressions:
            - key: gpu.node
              operator: In
              values:
              - "true"
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - H100
          - weight: 50
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-1a

        # 2. Pod Affinity: Co-locate with cache
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - llama-3-70b-cache
            topologyKey: kubernetes.io/hostname

        # 3. Pod Anti-Affinity: Spread across nodes
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - llama-3-70b
            topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - llama-3-70b
              topologyKey: topology.kubernetes.io/zone

      # 4. Topology Spread Constraints: Balance across zones
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: llama-3-70b

      # Node selector for basic GPU requirement
      nodeSelector:
        gpu.node: "true"

      containers:
      - name: vllm-server
        image: vllm/vllm-openai:latest
        command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
        args:
        - --model=$(MODEL_NAME)
        - --tensor-parallel-size=$(TENSOR_PARALLEL_SIZE)
        - --max-model-len=$(MAX_MODEL_LEN)
        - --dtype=$(DTYPE)
        - --gpu-memory-utilization=$(GPU_MEMORY_UTILIZATION)
        - --host=0.0.0.0
        - --port=8000
        - --disable-log-requests
        envFrom:
        - configMapRef:
            name: llama-3-70b-config
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
        - name: REDIS_HOST
          value: "localhost"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        ports:
        - name: http
          containerPort: 8000
        - name: health
          containerPort: 8080
        resources:
          requests:
            nvidia.com/gpu: "4"
            cpu: "8"
            memory: "64Gi"
          limits:
            nvidia.com/gpu: "4"
            cpu: "16"
            memory: "128Gi"
        volumeMounts:
        - name: shm
          mountPath: /dev/shm
        - name: model-cache
          mountPath: /root/.cache
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30  # 5 minutes total

      volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 16Gi
      - name: model-cache
        emptyDir: {}

---
# HorizontalPodAutoscaler for auto-scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llama-3-70b-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llama-3-70b
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 30
      selectPolicy: Max

---
# PodDisruptionBudget for high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: llama-3-70b-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: llama-3-70b

---
# ServiceMonitor for Prometheus monitoring
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llama-3-70b-monitor
  labels:
    app: llama-3-70b
spec:
  selector:
    matchLabels:
      app: llama-3-70b
  endpoints:
  - port: http
    interval: 30s
    path: /metrics

---
# Complete Scheduling Strategy Explained:
#
# ┌─────────────────────────────────────────────────────────────┐
# │  LLM Production Scheduling Strategy                         │
# │  ┌───────────────────────────────────────────────────────┐ │
# │  │ 1. Priority: 1000 (highest)                           │ │
# │  │    → Schedules first, never preempted                 │ │
# │  │                                                       │ │
# │  │ 2. Node Selector: gpu.node=true                       │ │
# │  │    → Must have GPU                                    │ │
# │  │                                                       │ │
# │  │ 3. Node Affinity (Required):                          │ │
# │  │    → Must have GPU node label                         │ │
# │  │                                                       │ │
# │  │ 4. Node Affinity (Preferred):                         │ │
# │  │    → Prefer H100 GPUs (weight: 100)                   │ │
# │  │    → Prefer zone us-west-1a (weight: 50)              │ │
# │  │                                                       │ │
# │  │ 5. Pod Affinity (Required):                           │ │
# │  │    → Co-locate with Redis cache on same node          │ │
# │  │    → Low latency cache access                         │ │
# │  │                                                       │ │
# │  │ 6. Pod Anti-Affinity (Required):                      │ │
# │  │    → Spread across different nodes                    │ │
# │  │    → No two pods on same node                         │ │
# │  │                                                       │ │
# │  │ 7. Pod Anti-Affinity (Preferred):                     │ │
# │  │    → Spread across zones (weight: 100)                │ │
# │  │    → High availability                                │ │
# │  │                                                       │ │
# │  │ 8. Topology Spread Constraints:                       │ │
# │  │    → Balance across zones (maxSkew: 1)               │ │
# │  │    → Even distribution                                │ │
# │  │                                                       │ │
# │  │ 9. Tolerations:                                       │ │
# │  │    → Can use dedicated GPU nodes                      │ │
# │  │    → Can use H100-only nodes                          │ │
# │  └───────────────────────────────────────────────────────┘ │
# └─────────────────────────────────────────────────────────────┘
#
# What This Achieves:
#
# ┌─────────────────────────────────────────────────────────────┐
# │  High Availability                                          │
# │  - Pods spread across different nodes                       │
# │  - Pods spread across different zones                       │
# │  - PDB ensures minimum 2 pods always available              │
# └─────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────┐
# │  Performance                                                │
# │  - Co-located with cache (low latency)                     │
# │  - Prefer H100 GPUs (fastest)                              │
# │  - Shared memory for multi-GPU                             │
# └─────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────┐
# │  Resource Efficiency                                        │
# │  - Binpack packing (prefer used nodes)                     │
# │  - Auto-scaling based on CPU/memory                        │
# │  - Graceful scale down                                     │
# └─────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────┐
# │  SLA Guarantee                                              │
# │  - Highest priority (never preempted)                      │
# │  - Production tolerations                                  │
# │  - Health checks and auto-restart                          │
# └─────────────────────────────────────────────────────────────┘
#
# Deployment Steps:
#
# 1. Label your nodes:
#    kubectl label nodes <gpu-node> gpu.node=true
#    kubectl label nodes <h100-node> nvidia.com/gpu.product=H100
#    kubectl label nodes <node> topology.kubernetes.io/zone=us-west-1a
#
# 2. Taint dedicated GPU nodes:
#    kubectl taint nodes <gpu-node> gpu-only=true:NoSchedule
#
# 3. Deploy everything:
#    kubectl apply -f 08-llm-model-scheduling.yaml
#
# 4. Verify scheduling:
#    kubectl get pods -o wide -l app=llama-3-70b
#    kubectl get pods -o wide -l app=llama-3-70b --sort-by=.spec.nodeName
#
# 5. Check zone distribution:
#    kubectl get pods -o wide -l app=llama-3-70b \
#      -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,ZONE:.metadata.labels.topology\.kubernetes\.io/zone
#
# 6. Test failover:
#    kubectl delete pod <pod-name>
#    # Watch it reschedule with same constraints
#
# 7. Test HPA:
#    kubectl autoscale deployment llama-3-70b --min=3 --max=10 --cpu-percent=70
#    # Load test and watch it scale up
#
# Monitoring:
#
# kubectl get pods -l app=llama-3-70b
# kubectl top pods -l app=llama-3-70b
# kubectl get hpa
# kubectl describe deployment llama-3-70b
#
# This configuration provides a production-ready, highly available,
# performant LLM serving deployment using all user-defined scheduling
# techniques available in Kubernetes!
